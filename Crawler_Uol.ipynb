{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler Uol",
      "provenance": [],
      "authorship_tag": "ABX9TyMHleMhrFHEDuRvT1hcJVlI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabselbach/TCC-implementacoes/blob/master/Crawler_Uol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI7C5dlBNgo9"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M7KHTv2Nd3v"
      },
      "source": [
        "!pip install selenium\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from nltk.tokenize import word_tokenize\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from google.colab import files\n",
        "from selenium import webdriver\n",
        "from urllib.error import HTTPError\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EpIW-37NsOf"
      },
      "source": [
        "# Clicar no botão para capturar mais propostas de redação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY_u5KloNznO"
      },
      "source": [
        "wd.get(\"https://educacao.uol.com.br/bancoderedacoes/\")\n",
        "element = wd.find_element_by_class_name('btn-primary')\n",
        "while not element == None:\n",
        "  try:\n",
        "    wd.execute_script(\"arguments[0].click();\", element)\n",
        "    element = wd.find_element_by_class_name('btn-primary')\n",
        "  except Exception as e:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGWZtbOGN3Rq"
      },
      "source": [
        "# Montando dataset com as redações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMfJM9B-OJRs"
      },
      "source": [
        "linksProposta=[]\n",
        "linksRedacao=[]\n",
        "DataRedacoes= []\n",
        "redacoes=[]\n",
        "data =[]\n",
        "contRedacao=0\n",
        "cont= 0\n",
        "items.pop(0)\n",
        "for i in items:  \n",
        "  linksProposta.append(i.get('href'))\n",
        "  linkprop = i.get('href')\n",
        "  if(cont<850):\n",
        "    try:\n",
        "      html = urlopen(linkprop)\n",
        "      soup = BeautifulSoup(html.read(),'html5lib')\n",
        "      tituloProposta = soup.find('i',{'class': 'custom-title'}).get_text()\n",
        "      textoMotivador = soup.find('div',{'class': 'text'})\n",
        "      val = soup.find_all('div', {'class': 'rt-line-option'})\n",
        "      if val != [] :\n",
        "        for j in val:\n",
        "          linkRedacao = j.find('a', href=re.compile(\"https://educacao.uol.com.br/bancoderedacoes/redacoes/\")).get('href')\n",
        "          print(linkRedacao)\n",
        "          try:\n",
        "            html2 = urlopen(linkRedacao)\n",
        "            soup2 = BeautifulSoup(html2.read(),'html5lib')\n",
        "            tituloRedacao = soup2.find('h2').get_text()\n",
        "            notaFinal = soup2.find('span', {'class': 'mark'})\n",
        "            notaComp1 = soup2.find('span', {'class': 'points'})\n",
        "            textoComTag = soup2.find('div', {'class': 'text-composition'})\n",
        "            textoResposta = soup2.find('div',{'class':'text'})\n",
        "            textoSemTag = re.sub('<span style=\"color:#00b050.>.*?<\\/span>', '', str(textoComTag))\n",
        "            ano = re.sub('<[^>]+?>', '', str(soup2.find('i', {'class': 'kicker'})))\n",
        "            ano = ano.split('/');\n",
        "            ano = ano[1]\n",
        "            temp = {\n",
        "              'ano': ano,\n",
        "              'tituloProposta':re.sub('<[^>]+?>', '', str(tituloProposta)),\n",
        "              'linkProposta':linkprop,\n",
        "              'textosMotivadores':re.sub('<[^>]+?>', '', str(textoMotivador)),\n",
        "              'tituloRedacao':tituloRedacao,\n",
        "              'linkRedacao':linkRedacao,\n",
        "              'notaFinal':re.sub('<[^>]+?>', '', str(notaFinal)),\n",
        "              'notaComp':re.sub('<[^>]+?>', '', str(notaComp1)),\n",
        "              'textoTotal':textoComTag,\n",
        "              try:\n",
        "                'correcao': PegaCorrecao(textoComTag),\n",
        "              except HTTPError as e:\n",
        "                'correcao': 'NAN',\n",
        "              'textoSemTag': re.sub('<[^>]+?>', '', str(textoSemTag)),\n",
        "              'textoResposta':re.sub('<[^>]+?>', '', str(textoResposta))\n",
        "            }\n",
        "            data.append(dict(temp))\n",
        "          except HTTPError as e:\n",
        "            content = e.read()\n",
        "    except HTTPError as e:\n",
        "      content = e.read()\n",
        "  cont=cont+1         \n",
        "df = pd.DataFrame(data, columns= ['ano', 'tituloProposta','linkProposta','textosMotivadores','tituloRedacao','linkRedacao','notaFinal','notaComp','textoTotal','correcao','textoSemTag','textoResposta'])\n",
        "df.to_csv('CrawlerUOL.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
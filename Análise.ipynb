{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Análise",
      "provenance": [],
      "collapsed_sections": [
        "QLrRpR3BHINX",
        "uSFp44kzHPo8"
      ],
      "authorship_tag": "ABX9TyMi0cOiW7+loTQ/+4oLqdNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabselbach/TCC-implementacoes/blob/master/An%C3%A1lise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLrRpR3BHINX"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4NVR4g9Gqvx"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install python-Levenshtein\n",
        "from Levenshtein import distance\n",
        "from openpyxl import load_workbook\n",
        "import re\n",
        "from re import match as re_match\n",
        "from re import compile as re_compile\n",
        "from google.colab import files\n",
        "from unicodedata import normalize\n",
        "import spacy\n",
        "import spacy.cli\n",
        "import ast\n",
        "spacy.cli.download('pt_core_news_sm')\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSFp44kzHPo8"
      },
      "source": [
        "# Função para filtrar correções falta de acento uol\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4iCQ6NcHYO6"
      },
      "source": [
        "def PegaCorrecao2(txt):\n",
        "  palavras=[]\n",
        "  p1 =''\n",
        "  p2=''\n",
        "  PalavrasEscritas = re.findall(r'<strong>(.*?)</strong>(.*?)<span style=\\\"color:#00b050\\\">(.*?)</span>',txt)\n",
        "  r = PalavrasEscritas.copy()\n",
        "  novo = []\n",
        "  val =[]\n",
        "  v=0\n",
        "  acento = re.compile('à|[á-ú]|ê|ô|ã|õ|í') \n",
        "  outro = re.compile(\"[,|:|?]\")\n",
        "  espaço = re.compile(\"^[[a-z]+\\s.]$\")\n",
        "  for palavra in r:\n",
        "    p1 = re.sub(' ','',palavra[0])\n",
        "    p2 = re.sub(' ','',palavra[2])\n",
        "    conteudo = nlp(palavra[0])\n",
        "    valores1 = conteudo.text.split()\n",
        "    conteudo = nlp(palavra[2])\n",
        "    valores2 = conteudo.text.split()\n",
        "    if(len(valores1)==len(valores2)):\n",
        "      i=0\n",
        "      for x in valores2:\n",
        "        if(distance(x,valores1[i])==1 and (acento.search(str(x))) and (not acento.search(str(valores1[i]))) ):\n",
        "          norm = normalize('NFKD',str(x)).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "          palavras.append(str(x))   \n",
        "        i=i+1\n",
        "  return palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zgx6DoqHe4i"
      },
      "source": [
        "# Criando o dataset das correções acentuais do UOL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkVRIc6qHkij"
      },
      "source": [
        "UOL = pd.read_csv('TodasReds.csv')\n",
        "SetUol=[]\n",
        "SetUol2=[]\n",
        "palavrasCorrigidasUOL=[]\n",
        "def isdigit(s):\n",
        "    comp = re_compile(\"^\\d+?\\.\\d+?$\")\n",
        "    if comp.match(s) is None:\n",
        "        return s.isdigit()\n",
        "    return True\n",
        "for (i,row) in UOL.iterrows():\n",
        "  if(i<101):\n",
        "    qtd=0\n",
        "    red = row['textoTotal']\n",
        "    redSotexto = row['textoSemTag']\n",
        "    manual = []\n",
        "    manual =PegaCorrecao2(red)\n",
        "    conteudo = nlp(redSotexto)\n",
        "    palavras = conteudo.text.split()\n",
        "    copytexto = palavras.copy()\n",
        "    palavrasComuns = {'publica','atração','acontece','antigas','pertence','pais','muita','econômico','sofre','ideia','coloca','mídia','para','correta','pública','integra','julga','justifica','mais','pela','fica','favorece','reforça','larga','esta','significa','disse','corte','começa','espera','merece','parece','formas','esquece'\t}\n",
        "    for x in copytexto:\n",
        "      if(len(x) < 2 or (x in palavrasComuns)):\n",
        "        palavras.remove(x)\n",
        "    Novo = []\n",
        "    l=0\n",
        "    for j in palavras:\n",
        "      \n",
        "      pnorm = normalize('NFKD',str(j)).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "      temp={\n",
        "        'correc': 0,\n",
        "        'palavra': j\n",
        "      }\n",
        "      copymanual = manual.copy();\n",
        "      Novo.append(temp)\n",
        "      for k in copymanual:\n",
        "        pNormManual = normalize('NFKD',str(k)).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "        if(pnorm == pNormManual and distance(str(j),str(k))==1 and len(str(j))>2):\n",
        "          Novo[l][\"correc\"]=1\n",
        "          if(str(j)==\"vem\"):\n",
        "            print(i)\n",
        "          palavrasCorrigidasUOL.append(Novo[l]['palavra'])\n",
        "          qtd +=1\n",
        "          manual.remove(k)\n",
        "          break\n",
        "      l +=1\n",
        "    Novo = pd.DataFrame(Novo)\n",
        "    temp2 = {\n",
        "      'texto': redSotexto,\n",
        "      'palavrasCorrec': Novo.copy(),\n",
        "      'qtd':qtd\n",
        "    }\n",
        "    SetUol2.append(temp2)\n",
        "  else:\n",
        "    break\n",
        "SetUol2 = pd.DataFrame(SetUol2)\n",
        "SetUol2.to_excel('setCorrecUol.xlsx', sheet_name='analiseRedações')\n",
        "files.download('setCorrecUol.xlsx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZF7M99AHobT"
      },
      "source": [
        "# Comparação das correções do Uol e do Acentua Fácil para obter os resultados, já calculando os vetores de real e predito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB1k5ebxH_Um"
      },
      "source": [
        "acentua = pd.read_excel('DICIO.xlsx')\n",
        "pd.set_option('display.max_rows', 3000)\n",
        "pd.set_option('display.max_columns', 20)\n",
        "pd.set_option('display.width', 3000000)\n",
        "red = []\n",
        "textosU = SetUol2['texto'] #texto normal\n",
        "palavrasU = SetUol2['palavrasCorrec'] \n",
        "m=0\n",
        "acento = re.compile('à|[á-ú]|ê|ô|ã|õ|í')\n",
        "real=[]\n",
        "predito=[]\n",
        "TP=0\n",
        "FP=0\n",
        "TN=0\n",
        "FN=0\n",
        "correcs=[]\n",
        "total=0\n",
        "redacoes=[]\n",
        "u=0\n",
        "DicioTudo=[]\n",
        "VopTP=[]\n",
        "palavrasCorrigidasErradas=[]\n",
        "palavrasNaoCorrigidas=[]\n",
        "pCorr=[]\n",
        "for (l,row) in acentua.iterrows(): \n",
        "  textoA =normalize('NFKD',row[\"texto\"]).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "  redAP = row[\"totalPalavras\"]\n",
        "  redUolP = pd.DataFrame(palavrasU[l].copy())\n",
        "  redAP = ast.literal_eval(redAP)\n",
        "  redAP = pd.DataFrame(redAP)\n",
        "  j=0\n",
        "  uolC=0\n",
        "  AcentuaC=0\n",
        "  y=0\n",
        "  for (j,row2) in redAP.iterrows():\n",
        "    total+=1\n",
        "    palavra = redUolP.loc[j,\"palavra\"]\n",
        "    tok =normalize('NFKD',redUolP.loc[j,\"palavra\"]).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "    correc1 = redUolP.loc[j,\"correc\"]\n",
        "    tok2 =normalize('NFKD',row2[\"palavra\"]).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "    correc2 = row2[\"correc\"]\n",
        "    real.append(correc1)\n",
        "    predito.append(correc2)\n",
        "    if(tok==tok2):\n",
        "        if(correc1==1 and correc2==1):\n",
        "          TP +=1\n",
        "        elif(correc1==0 and correc2==1):\n",
        "          FP +=1\n",
        "        elif(correc1==0 and correc2==0):\n",
        "          TN +=1\n",
        "        elif(correc1==1 and correc2==0):\n",
        "          DicioTudo.append(redUolP.loc[j,\"palavra\"])\n",
        "          FN +=1\n",
        "  redacoes.append(t) \n",
        "redacoes=pd.DataFrame(redacoes)\n",
        "print('valor de TP:',TP,'\\n')\n",
        "print('valor de FP:',FP,'\\n')\n",
        "print('valor de TN:',TN,'\\n')\n",
        "print('valor de FN:',FN,'\\n')\n",
        "print('qtd palavras',total) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}